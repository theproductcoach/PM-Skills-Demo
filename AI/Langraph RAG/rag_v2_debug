from setup_components import llm, vector_store

import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import SystemMessage
from langchain_core.tools import tool
from langgraph.graph import END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import MessagesState, StateGraph

# Debug function for easier toggling on/off of debug prints.
def debug_print(*args):
    print("[DEBUG]", *args)


# -------------------------------------------------------------------
# Step 0: Load documents and prepare them
# -------------------------------------------------------------------
debug_print("Initialising WebBaseLoader...")
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)

debug_print("Loading documents from the blog...")
docs = loader.load()
debug_print(f"Number of documents loaded: {len(docs)}")

debug_print("Splitting documents into chunks...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
debug_print(f"Number of chunks created: {len(all_splits)}")

# Index chunks
debug_print("Adding chunks to the vector store...")
_ = vector_store.add_documents(documents=all_splits)
debug_print("Chunks added to the vector store.")


# -------------------------------------------------------------------
# Step 1: Define the retrieval tool
# -------------------------------------------------------------------
@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve information related to a query."""
    debug_print("Inside 'retrieve' tool, received query:", query)
    retrieved_docs = vector_store.similarity_search(query, k=2)
    debug_print(f"Number of retrieved docs: {len(retrieved_docs)}")
    for idx, doc in enumerate(retrieved_docs):
        debug_print(f"Retrieved doc {idx} metadata:", doc.metadata)
        debug_print(f"Retrieved doc {idx} content length:", len(doc.page_content))

    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\n" f"Content: {doc.page_content}")
        for doc in retrieved_docs
    )
    debug_print("Returning retrieved document content from 'retrieve' tool.")
    return serialized, retrieved_docs


# -------------------------------------------------------------------
# Step 2: Generate an AIMessage which may include a tool-call
# -------------------------------------------------------------------
def query_or_respond(state: MessagesState):
    """Generate tool call for retrieval or respond."""
    debug_print("Entering 'query_or_respond' with current state messages:")
    for msg in state["messages"]:
        debug_print(" -", msg)

    llm_with_tools = llm.bind_tools([retrieve])
    debug_print("Calling LLM with the provided messages to possibly invoke a tool...")
    response = llm_with_tools.invoke(state["messages"])
    debug_print("LLM response received. Exiting 'query_or_respond'.")

    # The function must return a dict with "messages"
    return {"messages": [response]}


# -------------------------------------------------------------------
# Step 3: Create a ToolNode for tool execution
# -------------------------------------------------------------------
debug_print("Creating ToolNode with 'retrieve' tool...")
tools = ToolNode([retrieve])


# -------------------------------------------------------------------
# Step 4: Use retrieved content to generate a final answer
# -------------------------------------------------------------------
def generate(state: MessagesState):
    """Generate answer."""
    debug_print("Entering 'generate' with current state messages:")
    for msg in state["messages"]:
        debug_print(" -", msg)

    # Find the most recent tool messages
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]
    debug_print(f"Found {len(tool_messages)} tool message(s) in reverse order.")

    # Format into prompt
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        f"{docs_content}"
    )

    debug_print("System message content formed with retrieved documents.")
    conversation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]
    debug_print(f"Number of conversation messages to be used: {len(conversation_messages)}")

    prompt = [SystemMessage(system_message_content)] + conversation_messages
    debug_print("Prompt for LLM is ready. Invoking LLM...")

    response = llm.invoke(prompt)
    debug_print("LLM final response generated.")
    return {"messages": [response]}


# -------------------------------------------------------------------
# Step 5: Build and compile the graph
# -------------------------------------------------------------------
debug_print("Building the state graph...")
graph_builder = StateGraph(MessagesState)

debug_print("Adding nodes to graph...")
graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)

graph_builder.set_entry_point("query_or_respond")

debug_print("Setting up conditional edges for tool usage...")
graph_builder.add_conditional_edges(
    "query_or_respond",
    tools_condition,
    {END: END, "tools": "tools"},
)
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)

debug_print("Compiling the graph...")
graph = graph_builder.compile()


# -------------------------------------------------------------------
# Step 6: Save the graph visualisation as an image
# -------------------------------------------------------------------
debug_print("Saving graph visualisation as 'graph_visualisation.png'...")
with open("graph_visualisation.png", "wb") as f:
    f.write(graph.get_graph().draw_mermaid_png())
debug_print("Graph visualisation saved.")


# -------------------------------------------------------------------
# Step 7: Test the graph with an example input
# -------------------------------------------------------------------
input_message = "What is Task Decomposition?"
debug_print(f"Running the graph with input message: '{input_message}'")

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
    